---
title: "*Train* and *Test* Partitions"
author: "Alan T. Arnholt"
date: 'Last knit on `r format(Sys.time(), "%B %d, %Y at %X")`'
output: 
  bookdown::html_document2:
    theme: yeti
    highlight: textmate
---

<style>
body, h1, h2, h3, h4 {
    font-family: "Georgia", serif;
}

body {
    color: black;
}
a, a:hover {
    color: blue;
}
pre {
    font-size: 12px;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA, fig.align = 'center', fig.height = 5, fig.width = 5, 
                      prompt = TRUE, highlight = TRUE, tidy = FALSE, warning = FALSE, 
                      message = FALSE)
library(mdsr)
HELP <- na.omit(HELPrct)
# Parallel Processing
library(doMC)
registerDoMC(cores = 8)
```

# Partitions

When building a model used for prediction, one measure of the model's usefulness is how well it performs on unseen (future) data.  One method to evaluate a model on unseen data is to partition the data into a *`training set`* and a *`testing set`*.  The *`training set`* is the data set one uses to build the model.  The *`test set`* is the data one uses to evaluate the model's predictions.  Three approaches are used to create a *`training set`* and a *`test set`*.

The data set `HELPrct` after removing rows with missing values from the `mdsr` package is used to illustrate the three approaches with a 70/30 split of values.  That is, roughly 70% of the values will be used for the *`training set`* and the remainder will be used for the *`test set`*.  Note that there are `r nrow(HELP)` rows of data in `HELPrct` with complete information (no missing values), so a 70/30 split should result in roughly $`r nrow(HELP)`\times 0.70 \approx `r round(nrow(HELP)*.7,0)`$ values in the *`training set`* and the remaining `r nrow(HELP) - round(nrow(HELP)*.7,0)` values in the *`test set`*. 

* Using the `sample()` function (method 1):

```{r}
library(mdsr)
HELP <- na.omit(HELPrct)
set.seed(123)
trainIndex1 <- sample(x = c(TRUE, FALSE), size = nrow(HELP), replace = TRUE, prob = c(0.7, 0.3))
testIndex1 <- (!trainIndex1)
trainData1 <- HELP[trainIndex1, ]
testData1 <- HELP[testIndex1, ]
dim(HELP)
dim(trainData1)
dim(testData1)
# Train Fraction
dim(trainData1)[1]/dim(HELP)[1]
# Test Fraction
dim(testData1)[1]/dim(HELP)[1]
```

* Using the `sample()` function (method 2):

```{r}
set.seed(123)
trainIndex2 <- sample(x = 1:nrow(HELP), size = round(0.70*nrow(HELP), 0), replace = FALSE)
trainData2 <- HELP[trainIndex2, ]
testData2 <- HELP[-trainIndex2, ]
dim(trainData2)
dim(testData2)
# Train Fraction
dim(trainData2)[1]/dim(HELP)[1]
# Test Fraction
dim(testData2)[1]/dim(HELP)[1]
```

* Using `runif()`:

```{r}
set.seed(123)
HELP$Partition <- runif(dim(HELP)[1])
trainData3 <- subset(HELP, HELP$Partition > 0.3)
testData3 <- subset(HELP, HELP$Partition <= 0.3)
dim(trainData3)
dim(testData3)
# Train Fraction
dim(trainData3)[1]/dim(HELP)[1]
# Test Fraction
dim(testData3)[1]/dim(HELP)[1]
```

* Using the `createDataPartition` from the `caret` package:

```{r}
library(caret)  # load the caret package
set.seed(123)
trainIndex4 <- createDataPartition(y = HELP$homeless, p = .7, 
                                  list = FALSE, 
                                  times = 1)
trainData4 <- HELP[trainIndex4, ]
testData4 <- HELP[-trainIndex4, ]
dim(trainData4)
dim(testData4)
# Train Fraction
dim(trainData4)[1]/dim(HELP)[1]
# Test Fraction
dim(testData4)[1]/dim(HELP)[1]
```

**Note:**  When the `y` argument to `createDataPartition` is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data. 

## Building a model

In this section, a model is built using the *`training set`* to predict whether a person is `homeless`.

```{r}
model.null <- glm(homeless ~ 1, data = trainData4, family = binomial)
model.full <- glm(homeless ~ ., data = trainData4, family = binomial)
AICmod <- step(model.null, scope = list(lower = model.null, upper = model.full), direction = "both", data = trainData4)
summary(AICmod)
```
### Pseudo $R^2$

One can think of the pseudo $R^2$ for logistic regression as the analog to $R^2$ used in linear regression.  Pseudo $R^2$ measures how much of the deviance is explained by the model, where $R^2$ measures the variability explained by a linear model.

```{r}
resid.dev <- summary(AICmod)$deviance
resid.dev
null.dev <- summary(AICmod)$null.deviance
null.dev
PR2 <- 1 - (resid.dev / null.dev)
PR2 # Pseudo R squared
```

The model only explains `r round(PR2*100,2)`% of the deviance in the training data.  This will likely be less for the test data suggesting there are likely other factors that have not been measured that predict homelessness.

```{r}
testmod <- glm(homeless ~ i1 + sex + g1b + pss_fr + dayslink + sexrisk, family = binomial, data = testData4)
resid.dev <- summary(testmod)$deviance
resid.dev
null.dev <- summary(testmod)$null.deviance
null.dev
PR2 <- 1 - (resid.dev / null.dev)
PR2 # Pseudo R squared
# Note 'type = "response"' gives the predicted probabilities
PM <- predict(AICmod, newdata = testData4, type = "response")
pred <- ifelse(PM > 0.5, "homeless", "housed")
T1 <- table(pred, testData4$homeless)
T1
Accuracy <- sum(diag(T1))/sum(T1)
Accuracy
# Using caret
confusionMatrix(data = pred, reference = testData4$homeless)
```
See [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix) on Wikipedia.

## ROC 

An ROC curve is a really useful shortcut for summarizing the performance of a classifier over all possible thresholds. This saves one a lot of tedious work computing class predictions for many different thresholds and examining the confusion matrix for each threshold.

### AUC (Area Under the Curve)

* Single-number summary of model accuracy
* Summarizes performance accross all thresholds
* Rank different models within the same dataset

```{r}
library(caTools)
colAUC(PM, testData4$homeless, plotROC = TRUE)
library(pROC)
ROC <- roc(testData4$homeless, PM) # Note the order is switched here
plot(ROC, col = "blue")
auc(ROC)
```

An AUC of 0.5 is no better than random guessing, an AUC of 1.0 is a perfectly predictive model, and an AUC of 0.0 is perfectly anti-predictive (which rarely happens).  One can use the `trainControl()` function in `caret` to use AUC (instead of acccuracy), to tune the parameters of your models. The `twoClassSummary()` convenience function allows one to do this easily.

When using `twoClassSummary()`, be sure to always include the argument `classProbs = TRUE` or your model will throw an error! (You cannot calculate AUC with just class predictions. You need to have class probabilities as well.)

```{r}
# Create trainControl object: myControl
myControl <- trainControl(
  # method = "cv",
  # number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verbose = FALSE
)
#
# Train glm with custom trainControl: model
model <- train(homeless ~ ., data = trainData4, method = "glmStepAIC",
               trControl = myControl)

# Print model to console
model
summary(model)
```



## GLMNET

* `alpha = 0` $\rightarrow$ ridge
* `alpha = 1` $\rightarrow$ lasso

```{r}
set.seed(234)
# Create custom trainControl: myControl
myControl <- trainControl(
  method = "cv", number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)
# Train glmnet with custom trainControl and tuning: model
model <- train(
  homeless ~ ., data = trainData4,
  tuneGrid = expand.grid(alpha  = 0:1, lambda = seq(0.0001, 4, length = 20)),
  method = "glmnet",
  trControl = myControl
)
# Print model to console
model
# Print maximum ROC statistic
max(model[["results"]][["ROC"]])
plot(model)
```

### Imputation

* Median (`medianImpute`)
* knn (`knnImpute`)
* bagged tree model (`bagImpute`)

```{r, label = "MedImp"}
# Create custom trainControl: myControl
myControl <- trainControl(
  method = "cv", number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)

# Train glmnet with custom trainControl and tuning: model

model <- train(
  homeless ~ ., data = HELPrct, 
  tuneGrid = expand.grid(alpha  = 0:1, lambda = seq(0.0001, 100, length = 20)),
  method = "glmnet",
  metric = "ROC",
  trControl = myControl,
  preProcess = c("knnImpute"),
  na.action = na.pass
)

# Print model to console
model

# Print maximum ROC statistic
max(model[["results"]][["ROC"]])
plot(model)
plot(model$finalModel)
# getting coefficients
coef(model$finalModel, model$bestTune$lambda) 
```

## Working with ISLR data

### First Ridge

```{r}
library(ISLR)
library(caret)
Hitters <- na.omit(Hitters)
summary(Hitters)
dim(Hitters)
```


```{r}
set.seed(1)
myControl <- trainControl(
  method = "cv", 
  number = 10
)
hit.model <- train(
  Salary ~ ., data = Hitters,
  tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(3, -2, length = 100)),
  method = "glmnet",
  trControl = myControl
)
hit.model
coef(hit.model$finalModel, s = 11498)
coef(hit.model$finalModel, s = 705) 
coef(hit.model$finalModel, s = 50) 
plot(hit.model)
coef(hit.model$finalModel, hit.model$bestTune$lambda) 
hit.model$bestTune$lambda
```

Following ISLR, the data set is split into a training and a test set in order to estimate the error of each method.

```{r}
library(caret)
set.seed(52)
trainIndex <- createDataPartition(y = Hitters$Salary, p = 0.5,
                                list = FALSE,
                                times = 1)
HittersTrain <- Hitters[trainIndex, ]
HittersTest <- Hitters[-trainIndex, ]
dim(HittersTrain)
dim(HittersTest)
myControl <- trainControl(
  method = "repeatedcv", 
  repeats = 3
)
hit.modelT <- train(
  Salary ~ ., data = HittersTrain,
  tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(3, -2, length = 100)),
  method = "glmnet",
  trControl = myControl
)
hit.modelT
plot(hit.modelT)
coef(hit.modelT$finalModel, hit.modelT$bestTune$lambda) 
# Test RMSE error
preds <- predict(hit.modelT, HittersTest)
TestRMSE1 <- sqrt(mean((HittersTest$Salary - preds)^2))
TestRMSE1
```


### Next Lasso


```{r}
set.seed(14)
myControl <- trainControl(
  method = "repeatedcv", 
  repeats = 3
)
hit.modelT1 <- train(
  Salary ~ ., data = HittersTrain,
  tuneGrid = expand.grid(alpha = 1, lambda = 10^seq(3, -2, length = 100)),
  method = "glmnet",
  trControl = myControl
)
hit.modelT1
plot(hit.modelT1)
coef(hit.modelT1$finalModel, hit.model$bestTune$lambda)
# Test RMSE error
preds1 <- predict(hit.modelT1, HittersTest)
TestRMSE2 <- sqrt(mean((HittersTest$Salary - preds1)^2))
TestRMSE2
```

Note the test RMSE of the ridge model is `r TestRMSE1` while the test RMSE of the lasso model is `r TestRMSE2`.  In this scenario, the lasso has an advantage over the ridge model in terms of *parsimony* because 11 of the coefficients in the lasso model are exactly zero.

## Dichotomous Response Variable (Logistic Regression)
(Example taken from *An Introduction to Statistical Learning*)

> Goal:  Predict whether an individual will default on his or her credit card payment.

```{r}
library(ISLR)
head(Default)
ggplot(data = Default, aes(x = balance, y = income, colour = default)) + 
  geom_point(alpha = 0.8) + 
  theme_bw() + 
  scale_color_brewer(palette=7)
```

```{r}
ggplot(data = Default, aes(x = default, y = balance, fill = default)) + 
  geom_boxplot() + 
  theme_bw() + 
  guides(fill = FALSE) + 
  scale_fill_brewer(palette=7)
ggplot(data = Default, aes(x = default, y = income, fill = default)) + 
  geom_boxplot() + 
  theme_bw() + 
  guides(fill = FALSE) + 
  scale_fill_brewer(palette=7)
```

> Why not regression?

```{r}
Default$defaultN <- ifelse(Default$default == "No", 0, 1)
Default$studentN <- ifelse(Default$student =="No", 0, 1)
summary(Default$defaultN)
head(Default)
ggplot(data = Default, aes(x = balance, y = defaultN)) + 
  geom_point(alpha = 0.5) + 
  theme_bw() + 
  stat_smooth(method = "lm") +
  labs(y = "Probability of Default")
```

Some estimated probabilities are negative! For balances close to zero, we predict a negative probability of default.  For very large balances, we get values greater than 1. 

## Logistic regression

What we need is a function of $X$ that returns values between 0 and 1.  Consider the *logistic function*

$$p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}$$

which will always produce an *S-shaped* curve regardless of the value of $X$.


Let $y = \beta_0 + \beta_1X$, then

$p(X)=\frac{e^y}{1 + e^y} \rightarrow p(X) + e^{y}p(X) = e^{y} \rightarrow
p(X) = e^{y} - e^{y}p(X) \rightarrow \frac{p(X)}{1 - p(X)}=e^{y} = e^{\beta_0 + \beta_1X}$

$$\text{log}\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X$$

The quantity $\text{log}\left(\frac{p(X)}{1 - p(X)}\right)$ is called the *log-odds* or *logit*, and the quantity $\frac{p(X)}{1 - p(X)}$ is called the odds.  

NOTE:  In regression, $\beta_1$ gives the average change in $Y$ associated with a one-unit increase in $X$.  In a logistic regression model, increasing $X$ by one unit changes the log odds by $\beta_1$, or equivalently it multiplies the odds by $e^{\beta_1}$.

```{r}
ggplot(data = Default, aes(x = balance, y = defaultN)) + 
  geom_point(alpha = 0.5) + 
  theme_bw() + 
  stat_smooth(method = "glm", family = "binomial") +
  labs(y = "Probability of Default")
```

The probability of default given `balance` can be written as P(default = Yes | balance).

```{r}
log.mod <- glm(default ~ balance, data = Default, family = "binomial")
summary(log.mod)
```

From the output note that $\hat{\beta} = `r summary(log.mod)$coef[2, 1]`$.  This value indicates that an increase in `balance` is associated with an increase in the log odds of `default` by `r summary(log.mod)$coef[2, 1]` units.

## Making predictions

Using the estimated coefficients, the predicted probability of default for an individual with a balance of $1,500 is

$$\hat{p}(X) =  \frac{e^{\hat{\beta_0} + \hat{\beta}_1X}}{1 + e^{\hat{\beta_0} + \hat{\beta_1}X}} = \frac{e^{`r summary(log.mod)$coef[1, 1]`+ `r summary(log.mod)$coef[2, 1]` \times 1500}}{1 + e^{`r summary(log.mod)$coef[1, 1]`+ `r summary(log.mod)$coef[2, 1]` \times 1500}} = `r exp(predict(log.mod, newdata = data.frame(balance = 1500)))/(1 + exp(predict(log.mod, newdata = data.frame(balance = 1500))))`.$$

```{r}
predict(log.mod, newdata = data.frame(balance = 1500), type = "response")
```

For an individual with a `balance` of $2,500, the probability of default is

$$p(X) = `r exp(predict(log.mod, newdata = data.frame(balance = 2500)))/(1 + exp(predict(log.mod, newdata = data.frame(balance = 2500))))`$$.

```{r}
predict(log.mod, newdata = data.frame(balance = 2500), type = "response")
```

Are students more likely to default than non-students?

```{r}
mod2 <- glm(default ~ student, data = Default, family = "binomial")
summary(mod2)
```

Note that the coefficient associated with `studentYes` is positive, and the associated *p-value* is statistically significant.  This indicates that students tend to have higher default probabilities than non-students:

$$\widehat{Pr}(\text{default = Yes }|\text{ student = Yes}) = \frac{e^{`r summary(mod2)$coef[1, 1]`+ `r summary(mod2)$coef[2, 1]` \times 1}}{1 + e^{`r summary(mod2)$coef[1, 1]`+ `r summary(mod2)$coef[2, 1]` \times 1}} = `r exp(predict(mod2, newdata = data.frame(student = "Yes")))/(1 + exp(predict(mod2, newdata = data.frame(student = "Yes"))))`,$$

```{r}
predict(mod2, newdata = data.frame(student = "Yes"), type = "response")
```

$$\widehat{Pr}(\text{default = Yes }|\text{ student = No}) = \frac{e^{`r summary(mod2)$coef[1, 1]`+ `r summary(mod2)$coef[2, 1]` \times 0}}{1 + e^{`r summary(mod2)$coef[1, 1]`+ `r summary(mod2)$coef[2, 1]` \times 1}} = `r exp(predict(mod2, newdata = data.frame(student = "No")))/(1 + exp(predict(mod2, newdata = data.frame(student = "No"))))`.$$

```{r}
predict(mod2, newdata = data.frame(student = "No"), type = "response")
```


## Multiple Logistic Regression

Consider a model that uses `balance`, `income`, and `student` to predict `default`.

```{r}
mlog.mod <- glm(default ~ balance + income + student, data = Default, family = "binomial")
summary(mlog.mod)
```

There is a surprising result here.  The *p-value* associated with `balance` and the `student` are very small, indicating that each of these variables is associated with the probability of `default`.  However, the coefficient for `student` is negative, indicating that students are less likely to default than non-students.  Yet, the coefficient in `mod2` for `student` was positive.  How is it possible for the variable `student` to be associated with an *increase* in probability in `mod2` and a *decrease* in probability in `mlog.mod`?


```{r}
Default$PrDef <- exp(predict(mlog.mod))/(1 + exp(predict(mlog.mod)))
# Or the following is the same
Default$PRdef <- predict(mlog.mod, type = "response")
ggplot(data = Default, aes(x = balance, y = default, color = student)) + 
  geom_line(aes(x = balance, y = PrDef)) + 
  scale_color_brewer(palette = 7) + 
  theme_bw() +
  labs(y = "Probability of Default")
library(dplyr)
Default <- Default %>% 
  arrange(balance) %>% 
  mutate(balanceFAC = cut(balance, breaks = seq(500, 2655, length.out = 10), include.lower = TRUE))
head(Default)
T1 <- xtabs(~balanceFAC + default, data = Default, subset = student == "Yes")
T2 <- xtabs(~balanceFAC + default, data = Default, subset = student == "No")
SDR  <- T1[, 2]/apply(T1[, 1:2], 1, sum)
NSDR <- T2[, 2]/apply(T2[, 1:2], 1, sum)
BalanceM <- c(500+120, 739+120, 979+120, 1220+120, 1460+120, 1700+120, 1940+120, 2180+120, 2420+120)
DF <- data.frame(SDR, NSDR, BalanceM)
DF
ggplot(data = DF, aes(x = BalanceM, y = NSDR)) +
  geom_line(color = "red") + 
  geom_line(aes(x = BalanceM, y = SDR), color = "green") + 
  theme_bw() +
  labs(y = "Default Rate", x = "Credit Card Balance") 
```

## Odds Ratio (OR)

The *odds ratio*, denoted OR, is the ratio of the odds for $X = 1$ to the odds for $X = 0$, and is given by the equation

$$\text{OR}=\frac{\frac{p(1)}{1 - p(1)}}{\frac{p(0)}{1 - p(0)}}.$$

Substituting  
$$\frac{e^{\beta_0 + \beta_1}}{1 + e^{\beta_0 + \beta_1}}$$ for $p(X=1)$, and
$$\frac{e^{\beta_0}}{1 + e^{\beta_0}}$$ for $p(X=0)$, respectively, one can show that
$$\text{OR} = e^{\beta_1}.$$

The odds ratio is widely used as a measure of association as it approximates how much more likely or unlikely (in terms of odds) it is for the outcome to be present among those subjects with $X = 1$ as compared to those subjects with $X = 0$.  

Using the `mlog.mod` model, the response variable $Y$  is `default`, which denotes whether a credit card holder has defaulted (Yes/1) or not defaulted (No/0) on his/her current payment. The independent variable $X$  is `student`, which denotes whether the individual is a student (Yes/1) or is not a student (No/1).  A hypothetical odds ratio in this scenario of 2 is interpreted to mean that the odds of default among students is two times greater than the odds of default among non-students.  

An estimate of the odds ratio is

$$\widehat{\text{OR}} = e^{-0.6467758} = `r exp(coef(summary(mlog.mod))[4,1])`.$$

This is interpreted to mean that the odds of default among students is `r exp(coef(summary(mlog.mod))[4,1])` the odds of default among non-students.
